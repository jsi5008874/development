## 클라이언트 핸들링
레디스는 클라이언트 연결을 수락하는데 TCP 포트와 유닉스 소켓을 사용할 수 있다.
```
TCP 포트 VS 유닉스 소켓

TCP 포트 : 네트워크를 통한 연결, 원격 접속 가능
	특징
		1. IP 주소 + 포트 번호로 접속
		2. 같은 서버든 다른 서버든 상관없이 연결 가능
		3. 네트워크 계층을 거쳐 통신해서 유닉스에 비해 느림

유닉스 소켓 : 파일을 통한 연결
	특징
		1. 파일 경로로 접속
		2. 같은 서버에서만 연결 가능
		3. 네트워크를 안거쳐 TCP 보다 빠름
		4. 보안성 높음(외부에서 접근 불가)
>> 유닉스를 사용하려면 한 서버(인스턴스)에 redis와 애플리케이션(java, python등)이 설치되어야 한다.
```

일반적으로는 TCP 포트를 사용하지만 설정을 통해 유닉스 소켓 파일을 생성하여 사용할 수 있다.
```
unixsocket /tmp/redis.sock
unixcocketperm 777
>> 원하는 경로에 소켓 파일을 생성하고 소켓 통신을 활성화

redis-cli -s /tmp/redis.sock
>> 클라이언트가 해당 유닉스 소켓 파일의 경로를 활용해 레디스 서버에 연결 가능
```

레디스는 멀티플렉싱 방식을 사용하여 하나의 통신 채널을 통해 여러 데이터 스트림을 전송할 수 있다.
```
멀티플렉싱(Multiplexing) : 하나의 연결로 여러 요청을 처리
ex)
@Service
public class UserService {
    
    @Autowired
    private RedisTemplate<String, String> redisTemplate;
    
    public void processUsers() {
        // 모든 명령이 하나의 Redis 연결로 처리
        redisTemplate.opsForValue().set("user:1", "Alice");
        redisTemplate.opsForValue().set("user:2", "Bob");
        redisTemplate.opsForValue().get("user:1");
        redisTemplate.opsForValue().set("user:3", "Charlie");
        
        // Redis 입장에서는:
        // 연결 1개 --> SET, SET, GET, SET (순서대로 처리)
    }
}

커넥션 생성, 해제는 비용이 큰 작업인데 하나의 연결로 여러 명령을 전송해 서버 리소스 사용을 절약할 수 있다.


## Redis 내부 동작

Spring Boot                    Redis 서버
     |                              |
명령1 ─┐                             |
명령2 ─┼──> 하나의 TCP 연결 ─────> 큐에 순서대로 쌓임
명령3 ─┘                             |
                                 명령1 처리
                                 명령2 처리
                                 명령3 처리
                                    |
결과1 ←┐                             |
결과2 ←┼──────── 같은 연결로 응답 ───────┘
결과3 ←┘

```

하나의 스레드에서 여러 소켓을 감시하고 소켓 이벤트가 발생하는지 지속적으로 확인하여 효율적인 다중 클라이언트 지원이 가능
또한 논블로킹 I/O를 활용해 클라이언트 요청을 비동기적으로 처리하고 다수의 클라이언트 요청을 동시에 처리할 수 있다.

레디스는 클라이언트 커넥션을 생성할 때 TCP_NODELAY 옵션을 사용하는데
이 옵션을 설정하면 소켓은 작은 데이터라도 버퍼링하지 않고 지연 없이 가능한 빨리 패킷을 전송하려고 시도한다.
```
레디스 7 버전을 기준으로 레디스와 클라이언트가 통신할 때 TCP_NODELAY 옵션을 해제할 수 없다.
하지만 노드 간 복제 연결을 할 때는 옵션을 해제할 수 있다.
repl-disable-tcp-nodelay yes
>> yes로 하면 해제 가능

해당 설정값을 yes로 설정하면 복제본에 데이터를 전송할 때 더 작은 수의 TCP 패킷과 대역폭을 사용하며 이 때 복제본은 약 40ms의 지연이 발생할 수 있다.
```

### 클라이언트 버퍼 제한
#### 클라이언트 출력 버퍼
레디스는 클라이언트에 반환할 데이터를 임시로 저장하기 위해 각 클라이언트마다 클라이언트 출력 버퍼를 생성한다.
	1. 클라이언트 갯수에 따라 출력 버퍼를 생성
	2. 출력 버퍼는 반환할 데이터 양에 따라 가변으로 길이가 변경
하지만 클라이언트가 데이터 처리를 못하고 계속 요청을 보내면 출력 버퍼의 크기가 길어져 메모리 사용량이 늘어난다.

따라서 레디스는 출력 버퍼 크기에 제한을 둬서 버퍼 크기가 일정 수준 이상으로 증가할 경우 클라이언트를 종료한다.

**하드 제한, 소프트 제한**
하드 제한 : 고정된 제한 값으로 버퍼가 제한값에 도달하면 클라이언트 연결 종료
소프트 제한 : 시간에 따라 다른 값으로 예를 들면 10초 동안 지속적으로 32MB 보다 큰 경우 연결을 종료

**클라이언트 종류별 차이점**
일반 클라이언트 : 크기 제한 0
pub/sub 클라이언트 : 하드 제한 32MB, 소프트 제한 60초당 8MB
복제본 : 하드제한 256MB, 소프트 제한 60초당 64MB

**설정 변경**
CONFIG SET으로 버퍼 제한 설정을 변경할 수 있다.
```
CONFIG SET client-output-buffer-limit <class> <hard-limit> <soft-limit> <soft-limit-duration>
>> class는 normal(일반), slave(복제본), pubsub 중 하나이다.
>> hard-limit, soft-limit는 바이트 단위로 지정
>> soft-limit-duration은 초 단위로 지정
```

#### 클라이언트 쿼리 버퍼
클라이언트에서 받은 커맨드를 레디스가 임시로 보관하는 버퍼
크기는 기본 1GB로 버그로 인해 무한히 증가하는 것을 방지하기 위한 조치

**설정 변경**
client-query-buffer-limit 설정으로 변경

### 클라이언트 이빅션
레디스 7.0부터는 maxmemory-clients 설정값을 사용해 모든 클라이언트 연결이 사용하는 누적 메모리 양을 제한할 수 있다.
임계치에 도달하면 레디스는 서버에서 클라이언트 연결을 해제해 메모리를 확보한다.
가장 많은 메모리를 사용하는 연결부터 해제하려고 시도하며 이 기능을 클라이언트 이빅션이라 한다.
```
maxmemory-policy와 maxmemory-clients 설정의 차이점

maxmemory-policy : Redis에 저장된 '데이터'의 총량 제한
maxmemory-clients : 클라이언트와 통신하는 '버퍼' 메모리 제한

[Redis 서버 전체 메모리]
┌────────────────────────────────────┐
│                                    │
│  ┌──────────────────────┐          │
│  │  실제 데이터            │          │
│  │  (Keys & Values)     │          │ ← maxmemory 제한
│  │  user:1, product:2   │          │
│  └──────────────────────┘          │
│                                    │
│  ┌──────────────────────┐          │
│  │  클라이언트 버퍼         │          │
│  │  ├─ 클라A 버퍼         │          │ ← maxmemory-clients 제한
│  │  ├─ 클라B 버퍼         │          │
│  │  └─ 클라C 버퍼         │          │
│  └──────────────────────┘          │
│                                    │
│  + 기타 (복제, AOF 등)                │
│                                    │
└────────────────────────────────────┘
```

#### 설정 변경
```
maxmemory-clients 1G
>> 바이트 단위의 특정 크기로 직접 지정
maxmemory-clients 5%
>> maxmemory의 퍼센트로 설정
```
대규모 트래픽 환경에서는 클라이언트 연결에 사용되는 메모리 양을 % 단위로 설정하는 것을 권장

복제 연결에 사용되는 복제본과 마스터 커넥션은 클라이언트 이빅션의 영향을 받지 않는다.

특정 클라이언트 연결을 클라이언트 이빅션 기능에서 제외시킬 수 있다.
```
CLIENT NO-EVICT on
OK
>> 해당 명령을 사용하면 이빅션 기능에서 제외된다.
```

### Timeout과 TCP Keepalive

#### Timeout 설정
```
CONFIG SET timeout 600
OK
>> timeout을 600초로 설정
```
장기간 동안 커맨드를 수행하지 않는 유휴 클라이언트를 제거할 수 있음
유휴 클라이언트가 쌓여서 서비스 장애가 발생하는 상황을 방지
이 설정은 pub/sub 클라이언트에는 영향을 주지 않는다.

#### TCP Keepalive
연결된 클라이언트에게 주기적으로 TCP ACK를 보내고 클라이언트로부터 응답이 없는 경우에 연결을 끊는 설정
타임아웃과 달리 실제로 클라이언트가 정상적으로 응답할 수 있는 상태인지를 우선 확인하고 응답이 없을 때 끊는 방식

기본적으로 300초로 설정되어 있으며 레디스는 연결된 클라이언트에게 5분마다 TCP ACK을 보낸다.

## 파이프 라이닝
파이프 라이닝은 클라이언트가 연속적으로 여러 개의 커맨드를 레디스 서버에 보낼 수 있도록 하는 기능

일반적으로 클라이언트는 하나의 커맨드를 보내고 응답을 받지만 파이프 라이닝은 한 번에 여러 개의 커맨드를 보내 일괄 처리하기 때문에
레디스의 응답 속도를 줄이고 처리량을 늘릴 수 있다.

```
$ (printf "PING\r\nPING\r\nPING\r\n"; sleep 1) | nc localhost 6379
+PONG
+PONG
+PONG
>> 레디스 서버에 줄바꿈을 이용해 여러 커맨드를 보내면 된다.
```

또한 레디스 클라이언트에서도 사용할 수 있다.
```
## java 예시

@Service
public class RedisPipelineService {
    
    @Autowired
    private RedisTemplate<String, String> redisTemplate;
    
    public List<Object> pipelineExample() {
        // 파이프라이닝 사용
        return redisTemplate.executePipelined(
            new RedisCallback<Object>() {
                @Override
                public Object doInRedis(RedisConnection connection) {
                    // 여러 명령을 한번에 보냄
                    connection.ping();
                    connection.ping();
                    connection.ping();
                    return null;
                }
            }
        );
        // 결과: [PONG, PONG, PONG]
    }
}
```
파이프 라이닝을 사용하면 왕복 시간을 줄일 수 있을 뿐만 아니라 처리량도 크게 향상시킬 수 있다.

레디스 서버가 응답하기 위해 소켓 I/O를 수행할 때 운영체제 커널의 read(), write() 시스템 콜 호출하는 과정에서 생기는 레이턴시 증가가
레디스 서버에서 데이터를 찾고 반환하는 과정보다 크다.
즉 파이프 라이닝을 통해 한번에 여러 커맨드를 수행하면 커널의 read(), write() 시스템 콜 호출이 줄어서 처리량을 줄일 수 있다.


p.368 ~ 371의 파이썬을 이용한 테스트를 확인해보면 파이프 라이닝을 통해 엄청난 성능 향상이 생기는 것을 알 수 있다.
단지 데이터 수집 알고리즘을 파이프 라인으로 변경했을 뿐인데 142배의 성능 향상이 되었다.
![[KakaoTalk_Photo_2025-10-21-23-07-54 002.jpeg|725]]
첫 번째 케이스(파이프 라이닝 적용 X)는 약 오후 7시 ~ 오후 11시까지 약 4시간 정도 진행됐으며 평균적으로 분당 800개의 커맨드를 처리했지만
두 번째 케이스(파이프 라이닝 적요 O)는 약 4분간 평균 66,000개의 커맨드를 처리했다.
이렇게 파이프 라이닝을 활용해 네트워크 I/O를 최소화하면 상당한 성능 향상을 이룰 수 있다.

하지만 한 번에 너무 많은 쿼리를 파이프라인을 이용해 처리하면 네트워크 대역폭 한계로 인해 속도가 저하될 수 있으며
레디스의 클라이언트 쿼리 버퍼 제한에 걸려 오류가 발생할 수도 있다.

따라서 커맨드를 일정한 개수로 나누어 배치 형태로 서버에 보내는 것이 좋으며 배치 사이즈는 충분한 테스트를 통해 결정해야 한다.

레디스가 파이프 라인으로 들어온 명령을 처리할 때 하나의 파이프 라인에 속하더라도 원자성을 보장하진 않는다.
![[KakaoTalk_Photo_2025-10-21-23-07-54 001.jpeg|725]]
클라이언트 1이 먼저 서버에 접근해 커맨드를 실행하고 있더라도
클라이언트 2의 신규 연결을 차단하지 않으며 각각의 명령은 다른 연결 사이에서 교차로 수행될 수 있다.

파이프라이닝을 이용한 연결은 트랜잭션의 개념이 아니기 때문에 커맨드에 오류가 발생해도 전체적인 롤백이 일어나진 않는다.
대신 오류를 발생시킨 커맨드만 수행되지 않고 나머지는 정상적으로 수행된다.

## 클라이언트 사이드 캐싱
레디스 버전 6에서 클라이언트 사이드에서 캐싱 기능이 추가됐다.

레디스와 클라이언트 사이의 통신에서 가장 많은 시간을 차지하는 것이 네트워크 I/O, 즉 왕복 시간으로 이 시간을 줄이는 것이 성능 향상의 방법 중 하나이다.
그 중 하나가 캐싱이며 이는 쿼리가 들어올 때마다 레디스 서버에 데이터를 요청하는 대신 클라이언트 측에서 데이터를 캐싱하여 데이터를 반환하는 방법이다.

일반적으로는 애플리케이션 서버가 레디스에 데이터를 요청한 후 응답을 받아오지만
이 방법은 자주 사용되는 쿼리의 경우 클라이언트 사이드에서 캐싱을 해서 레디스 서버와 네트워크 I/O를 거치지 않아도 되도록 한다.

하지만 캐싱할 때 문제점인 데이터 정합성에 대해 고려해야한다.
레디스의 클라이언트 사이드 캐싱에서는 이를 처리하는 방법을 트래킹이라고 부르며 두 가지의 방법이 있다.

#### 기본 모드 트래킹
기본 모드에서는 레디스 서버가 클라이언트가 액세스한 키를 기억해서 동일한 키가 수정될 때 무효 메시지를 전송
레디스 서버에서 이를 기억해야해서 메모리 비용이 들지만 정확하게 클라이언트가 갖고 있는 키에 대해서만 무효한 메세지를 보낼 수 있다.

#### 브로드캐스팅 모드 트래킹
레디스 서버가 모든 키에 대한 액세스를 기억하려고 시도하지 않으며 특정 프리픽스에 접근한 클라이언트만 기억한다.
따라서 기본 모드 트래킹보다 레디스 서버의 메모리 사용량이 적다.
대신 클라이언트는 특정 프리픽스를 가진 키를 기억해야 하며 자신이 소유하지 않은 키라 하더라도 해당 프리픽스와 일치하는 키가
변경될 때마다 변경 메시지를 수신하는 단점이 존재하며 이로 인해 레디스 서버는 CPU를 많이 소비할 수 있다.
```
## 레디스 서버 CPU 사용량이 많아지는 이유

브로드캐스팅 모드 트래킹은 기본 모드에 비해 변경 메시지를 많이 전송해야 한다.
ex)
기본 모드
클라이언트A가 읽은 키: user:1, user:5 
클라이언트B가 읽은 키: user:2, user:10 
클라이언트C가 읽은 키: product:100 
user:1 변경됨! → 클라이언트A에게만 알림 (정확!) → CPU: 메시지 1개만 전송

브래도 캐스팅 모드
클라이언트A가 관심있는 프리픽스: user:* 
클라이언트B가 관심있는 프리픽스: user:* 
클라이언트C가 관심있는 프리픽스: product:* 
user:1 변경됨! → user:* 프리픽스를 추적하는 모든 클라이언트에게 알림 → 클라이언트A에게 알림 (필요함) 
→ 클라이언트B에게 알림 (불필요함) → CPU: 메시지 2개 전송 (1개는 낭비)

>> 이렇게 브로드 캐스팅 모드는 프리픽스를 추적하는 모든 클라이언트에게 보내야해서 더 많은 메시지 전송을 야기하는 방식이다.


## 메시지 전송은 I/O bound 작업인데 CPU 사용량과 무슨 상관이지?
1. 메시지 생성 및 직렬화 ← CPU 사용
2. 출력 버퍼에 쓰기 ← CPU 사용
3. 클라이언트별 반복 ← CPU 사용
4. 네트워크 전송 ← I/O bound (비동기)  
>> 실제 메시지 전송은 I/O bound가 맞지만 1~3번 과정에서 CPU를 많이 사용
```